# LLM-Cache-papers
LLM Sematic Cache Papers

1. **[GPTCache]** Fu Bang. [GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings](https://aclanthology.org/2023.nlposs-1.24/) The 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023) [Github page](https://github.com/zilliztech/GPTCache)
2. **[IC-Cache]** Yifan Yu, Yu Gan, Nikhil Sarda, Lillian Tsai, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Hank Levy, David E. Culler: [IC-Cache: Efficient Large Language Model Serving via In-context Caching](https://arxiv.org/pdf/2501.12689v3). SOSP 2025: 375-398 **(CCF A)**
3. **[vCache]** Luis Gaspar Schroeder, Aditya Desai, Alejandro Cuadron, Kyle Chu, Shu Liu, Mark Zhao, Stephan Krusche, Alfons Kemper, Ion Stoica, Matei Zaharia, Joseph E. Gonzalez: [vCache: Verified Semantic Prompt Caching](https://arxiv.org/pdf/2502.03771), aCoRR abs/2502.03771 (2025) [Github page](https://github.com/vcache-project/vCache)
4. **[MeanCache]** Waris Gill, Mohamed Elidrisi, Pallavi Kalapatapu, Ammar Ahmed, Ali Anwar, Muhammad Ali Gulzar: [MeanCache: User-Centric Semantic Caching for LLM Web Services](https://arxiv.org/pdf/2403.02694). IPDPS 2025: 1298-1310 [Github page](https://github.com/warisgill/MeanCache) **(CCF B)**
5. Camille Couturier, Spyros Mastorakis, Haiying Shen, Saravan Rajmohan, Victor RÃ¼hle: [Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models](https://arxiv.org/pdf/2505.11271). ICCCN 2025
6. Yueyue Liu, Hongyu Zhang, Yuantian Miao: [SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization](https://arxiv.org/pdf/2508.03258). CoRR abs/2508.03258 (2025)
7. Xutong Liu, Baran Atalar, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, John C. S. Lui, Wei Chen, Carlee Joe-Wong: [Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation](https://arxiv.org/pdf/2508.07675). CoRR abs/2508.07675 (2025)
8. Arun Iyengar, Ashish Kundu, Ramana Kompella, Sai Nandan Mamidi: [A Generative Caching System for Large Language Models](https://arxiv.org/pdf/2503.17603). CoRR abs/2503.17603 (2025)
9. Keihan Haqiq, Majid Vafaei Jahan, Saeede Anbaee Farimani, Seyed Mahmood Fattahi Masoom: MinCache: A hybrid cache system for efficient chatbots with hierarchical embedding matching and LLM. Future Gener. Comput. Syst. 170: 107822 (2025)
10. Sajal Regmi, Chetan Phakami Pun: [GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic Embedding Caching](https://arxiv.org/pdf/2411.05276). CoRR abs/2411.05276 (2024) [Codes](https://www.npmjs.com/package/gpt-semantic-cache), [Dataset](https://github.com/sajalregmi/gpt-semantic-cache-test/tree/main/test_dataset), [Test Scripts](https://github.com/sajalregmi/gpt-semantic-cache-test/blob/main/test.ts)
11. **[ContextCache]** Jianxin Yan, Wangze Ni, Lei Chen, Xuemin Lin, Peng Cheng, Zhan Qin, Kui Ren: ContextCache: [Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://www.vldb.org/pvldb/vol18/p5391-yan.pdf) ([demo](https://youtu.be/R3NByaQS7Ws)). Proc. VLDB Endow. 18(12): 5391-5394 (2025) [Github page](https://github.com/uYanJX/ContextCache) **(CCF A)**
12. **[GenCache]** Sarthak Chakraborty, Suman Nath, Xuchao Zhang, Chetan Bansal, Indranil Gupta: [Generative Caching for Structurally Similar Prompts and Responses](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/GenCache_NeurIPS25.pdf), NeurIPS 2025 **(CCF A)**

