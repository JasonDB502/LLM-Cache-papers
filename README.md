# LLM-Cache-papers
LLM Sematic Cache Papers

1. **[GPTCache]** Fu Bang. [GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings](https://aclanthology.org/2023.nlposs-1.24/) The 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023) [Github page](https://github.com/zilliztech/GPTCache)
2. **[IC-Cache]** Yifan Yu, Yu Gan, Nikhil Sarda, Lillian Tsai, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Hank Levy, David E. Culler: [IC-Cache: Efficient Large Language Model Serving via In-context Caching](https://arxiv.org/pdf/2501.12689v3). SOSP 2025: 375-398
3. **[vCache]** Luis Gaspar Schroeder, Aditya Desai, Alejandro Cuadron, Kyle Chu, Shu Liu, Mark Zhao, Stephan Krusche, Alfons Kemper, Ion Stoica, Matei Zaharia, Joseph E. Gonzalez: [vCache: Verified Semantic Prompt Caching](https://arxiv.org/pdf/2502.03771), aCoRR abs/2502.03771 (2025)
4. Camille Couturier, Spyros Mastorakis, Haiying Shen, Saravan Rajmohan, Victor RÃ¼hle: [Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models](https://arxiv.org/pdf/2505.11271). ICCCN 2025
5. Yueyue Liu, Hongyu Zhang, Yuantian Miao: [SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization](https://arxiv.org/pdf/2508.03258). CoRR abs/2508.03258 (2025)

