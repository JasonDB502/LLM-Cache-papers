# LLM-Cache-papers
LLM Sematic Cache Papers

1. **[GPTCache]** Fu Bang. [GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings](https://aclanthology.org/2023.nlposs-1.24/) The 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023) [Github page](https://github.com/zilliztech/GPTCache)
2. **[IC-Cache]** Yifan Yu, Yu Gan, Nikhil Sarda, Lillian Tsai, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Hank Levy, David E. Culler: [IC-Cache: Efficient Large Language Model Serving via In-context Caching](https://arxiv.org/pdf/2501.12689v3). SOSP 2025: 375-398
3. **[vCache]** Luis Gaspar Schroeder, Aditya Desai, Alejandro Cuadron, Kyle Chu, Shu Liu, Mark Zhao, Stephan Krusche, Alfons Kemper, Ion Stoica, Matei Zaharia, Joseph E. Gonzalez: [vCache: Verified Semantic Prompt Caching](https://arxiv.org/pdf/2502.03771), aCoRR abs/2502.03771 (2025) [Github page](https://github.com/vcache-project/vCache)
4. Camille Couturier, Spyros Mastorakis, Haiying Shen, Saravan Rajmohan, Victor RÃ¼hle: [Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models](https://arxiv.org/pdf/2505.11271). ICCCN 2025
5. Yueyue Liu, Hongyu Zhang, Yuantian Miao: [SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization](https://arxiv.org/pdf/2508.03258). CoRR abs/2508.03258 (2025)
6. Xutong Liu, Baran Atalar, Xiangxiang Dai, Jinhang Zuo, Siwei Wang, John C. S. Lui, Wei Chen, Carlee Joe-Wong: [Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation](https://arxiv.org/pdf/2508.07675). CoRR abs/2508.07675 (2025)
7. Waris Gill, Mohamed Elidrisi, Pallavi Kalapatapu, Ammar Ahmed, Ali Anwar, Muhammad Ali Gulzar: [MeanCache: User-Centric Semantic Caching for LLM Web Services](https://arxiv.org/pdf/2403.02694). IPDPS 2025: 1298-1310 [Github page](https://github.com/warisgill/MeanCache)

